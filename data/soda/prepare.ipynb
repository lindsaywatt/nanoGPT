{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ba51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = load_dataset(\"allenai/soda\")\n",
    "del dataset_full['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_base_tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokeniser = tiktoken.Encoding(\n",
    "    name=\"gpt2_soda\",\n",
    "    pat_str=gpt2_base_tokeniser._pat_str,\n",
    "    mergeable_ranks=gpt2_base_tokeniser._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **gpt2_base_tokeniser._special_tokens,\n",
    "        \"<|sep|>\": 50257,\n",
    "        \"<|turn|>\": 50258\n",
    "    }\n",
    ")\n",
    "\n",
    "encode = lambda s: tokeniser.encode(s, allowed_special=tokeniser.special_tokens_set)\n",
    "decode = lambda s: tokeniser.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65221967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(item):\n",
    "    speakers = [e.title() for e in item['speakers'][0]]\n",
    "    unique_speakers = list(set(speakers))\n",
    "    dialog = item['dialogue'][0]\n",
    "\n",
    "    context_str = (\n",
    "        f\"{item['narrative'][0]}<|sep|>The following is a conversation in the scene between \"\n",
    "        f\"{unique_speakers[0]} and {unique_speakers[1]}<|sep|>\"\n",
    "    )\n",
    "    \n",
    "    x_tokens = []\n",
    "    y_tokens = []\n",
    "    first_speaker = speakers[0]\n",
    "    last_x = context_str\n",
    "    for i, (speaker, dialog_line) in enumerate(zip(speakers, dialog)):\n",
    "        if speaker == first_speaker:\n",
    "            last_x += f\"{dialog_line}<|turn|>\"\n",
    "            continue\n",
    "        \n",
    "        x = last_x\n",
    "        y = f\"{dialog_line}\"\n",
    "        \n",
    "        last_x += f\"{dialog_line}<|turn|>\"\n",
    "        \n",
    "        x_tokens.append(encode(x))\n",
    "        y_tokens.append(encode(y))\n",
    "\n",
    "    return {'x': x_tokens, 'y': y_tokens, 'x_size': [len(e) for e in x_tokens], 'y_size': [len(e) for e in y_tokens]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a27f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenised = dataset_full.map(\n",
    "    process,\n",
    "    num_proc=mp.cpu_count(),\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_tokenised)\n",
    "print(dataset_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('~/nanoGPT/data/soda/').expanduser()\n",
    "\n",
    "for split, dataset in dataset_tokenised.items():\n",
    "    x_total_length = np.sum(dataset['x_size'])\n",
    "    y_total_length = np.sum(dataset['y_size'])\n",
    "\n",
    "    x_filename = data_dir / f'{split}_x.bin'\n",
    "    y_filename = data_dir / f'{split}_y.bin'\n",
    "    x_size_filename = data_dir / f'{split}_x_lengths.bin'\n",
    "    y_size_filename = data_dir / f'{split}_y_lengths.bin'\n",
    "    \n",
    "    x_array = np.memmap(x_filename, dtype=np.uint16, mode='w+', shape=(x_total_length,))\n",
    "    y_array = np.memmap(y_filename, dtype=np.uint16, mode='w+', shape=(y_total_length,))\n",
    "    x_size_array = np.memmap(x_size_filename, dtype=int, mode='w+', shape=(len(dataset), 2))\n",
    "    y_size_array = np.memmap(y_size_filename, dtype=int, mode='w+', shape=(len(dataset), 2))\n",
    "\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    x_size_idx = 0\n",
    "    y_size_idx = 0\n",
    "    for example in tqdm.tqdm(dataset, unit='examples', smoothing=0.01):\n",
    "        x = example['x']\n",
    "        y = example['y']\n",
    "        x_size = len(x)\n",
    "        y_size = len(y)\n",
    "        \n",
    "        x_start_index = x_idx\n",
    "        y_start_index = y_idx\n",
    "        x_end_index = x_start_index + x_size\n",
    "        y_end_index = y_start_index + y_size\n",
    "\n",
    "        if x_end_index > np.iinfo(int).max:\n",
    "            print(f'index int too big! value: `{x_end_index:,}` > {np.iinfo(int).max:,}')\n",
    "            break\n",
    "            \n",
    "        if y_end_index > np.iinfo(int).max:\n",
    "            print(f'index int too big! value: `{y_end_index:,}` > {np.iinfo(int).max:,}')\n",
    "            break\n",
    "    \n",
    "        x_array[x_start_index : x_end_index] = x\n",
    "        y_array[y_start_index : y_end_index] = y\n",
    "        x_size_array[x_size_idx, :] = [x_start_index, x_end_index]\n",
    "        y_size_array[y_size_idx, :] = [y_start_index, y_end_index]\n",
    "        \n",
    "        x_idx += x_size\n",
    "        y_idx += y_size\n",
    "        x_size_idx += 1\n",
    "        y_size_idx += 1\n",
    "\n",
    "    x_array.flush()\n",
    "    y_array.flush()\n",
    "    x_size_array.flush()\n",
    "    y_size_array.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "\n",
    "x_data = np.memmap(f'{split}_x.bin', dtype=np.uint16, mode='r')\n",
    "y_data = np.memmap(f'{split}_y.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "x_size = np.memmap(f'{split}_x_lengths.bin', dtype=int, mode='r')\n",
    "y_size = np.memmap(f'{split}_y_lengths.bin', dtype=int, mode='r')\n",
    "\n",
    "x_size = x_size.reshape((x_size.shape[0]//2, 2))\n",
    "y_size = y_size.reshape((y_size.shape[0]//2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a random example\n",
    "\n",
    "i = random.randint(0, x_size.shape[0]-1)\n",
    "\n",
    "print('example', i)\n",
    "\n",
    "x_idxs = x_size[i]\n",
    "y_idxs = y_size[i]\n",
    "\n",
    "x_item = x_data[x_idxs[0]:x_idxs[1]]\n",
    "y_item = y_data[y_idxs[0]:y_idxs[1]]\n",
    "\n",
    "\n",
    "x = decode(x_item)\n",
    "y = decode(y_item)\n",
    "\n",
    "print(f'input: `{x}`')\n",
    "print()\n",
    "print(f'target: `{y}`')\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c24274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c34c5edb625a87c574d7be569ef7477cc0d14c4aa962c376e7fff5991391c941"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
